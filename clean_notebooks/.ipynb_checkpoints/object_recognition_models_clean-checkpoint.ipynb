{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display plots inline\n",
    "%matplotlib notebook\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "from bambi import Model, Prior\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import pymc3_utils as pmu\n",
    "\n",
    "# suppress system warnings for legibility\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# resize plots to fit labels inside bounding box\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "# MPI color scheme\n",
    "sns.set(style='white', palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cambridge Recognition Memory Tasks\n",
    "## Loading participant-level predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reading = pd.read_csv('data/reading_intercepts.tsv', sep='\\t')[['pp', 'ravens_intercept', 'span_intercept', 'raw_reading_score', 'adjusted_reading_score']]\n",
    "display(df_reading.head().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading object memory data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start by loading the data\n",
    "df_comt = pd.read_csv('data/cambridge.tsv', sep='\\t').dropna()\n",
    "df_comt = df_comt.merge(df_reading, left_on='pp', right_on='pp').dropna()\n",
    "display(df_comt.head().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dummy coding, scaling, centering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize reading scores\n",
    "df_comt['raw_reading_score'] = pmu.standardize(df_comt['raw_reading_score'])\n",
    "df_comt['adjusted_reading_score'] = pmu.standardize(df_comt['adjusted_reading_score'])\n",
    "df_comt['ravens_intercept'] = pmu.standardize(df_comt['ravens_intercept'])\n",
    "df_comt['span_intercept'] = pmu.standardize(df_comt['span_intercept'])\n",
    "\n",
    "# create dummies for cars and faces, bikes will be the default\n",
    "df_comt['cars'] = pd.get_dummies(df_comt['category'])['cars']\n",
    "df_comt['faces'] = pd.get_dummies(df_comt['category'])['faces']\n",
    "df_comt['bikes'] = pd.get_dummies(df_comt['category'])['bikes']\n",
    "\n",
    "# create dummy for visual noise, no noise will be the default\n",
    "df_comt['noise'] = pd.get_dummies(df_comt['visual_noise'])['noise']\n",
    "\n",
    "# create dummy for learn trials, not learn will be the default\n",
    "df_comt['learn'] = pd.get_dummies(df_comt['learn_trial'])['learn']\n",
    "\n",
    "# dump learning trials\n",
    "df_comt = df_comt[df_comt['learn'] == 0]\n",
    "\n",
    "display(df_comt.head().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default model params\n",
    "defaults = {\n",
    "    'samples': 5000,\n",
    "    'tune': 2500,\n",
    "    'chains': 4,\n",
    "    'init': 'advi+adapt_diag',\n",
    "    'family': 'bernoulli',\n",
    "    'priors': {'fixed': 'narrow', 'random': 'narrow'},\n",
    "}\n",
    "# these models take a while to sample, so we'll store them in a model pickler in case we want to reuse them later\n",
    "# the pickled models and traces are too big to include in the git repository\n",
    "# so if you're reproducing this analysis you'll have to do the sampling yourself\n",
    "pickler = pmu.ModelPickler('object_recognition_models.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model0 = Model(df_comt)\n",
    "model0.fit('ACC ~ noise*cars + noise*faces',\n",
    "                           **defaults)\n",
    "pickler.add(model0, 'noise * type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(df_comt)\n",
    "model1.fit('ACC ~ noise*cars + noise*faces + raw_reading_score',\n",
    "                           **defaults)\n",
    "pickler.add(model1, 'noise * type + unadj_reading_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Model(df_comt)\n",
    "model2.fit('ACC ~ noise*cars + noise*faces + noise*raw_reading_score',\n",
    "                           **defaults)\n",
    "pickler.add(model2, 'noise * category + noise * unadj_reading')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Model(df_comt)\n",
    "model3.fit('ACC ~ noise*cars + noise*faces + cars*raw_reading_score + faces*raw_reading_score',\n",
    "                           **defaults)\n",
    "pickler.add(model3, 'noise * category + category * unadj_reading')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = Model(df_comt)\n",
    "model4.fit('ACC ~ noise*cars*raw_reading_score + noise*faces*raw_reading_score',\n",
    "                           **defaults)\n",
    "pickler.add(model4, 'noise * category * unadj_reading')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison (task by participant random slope models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_comparison, comparison = pmu.compare(list(pickler.models.values()), list(pickler.models.keys()), ic='LOO')\n",
    "plt.savefig('figures/object_recognition_model_comparison.pdf')\n",
    "plt.savefig('figures/object_recognition_model_comparison.png', dpi=600, bbox_inches='tight')\n",
    "display(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are looking for the best parsimonious model for the effect of literacy on recognition memory, rather looking for the most accurate predictive model (which we could get by model averaging), we are ignoring the weights column and instead selecting the single best-fit model.  \n",
    "The four best models in our model comparison are all within the standard error of the best-fit model, indicating highly similar model fit. In the absence of a decisive best-fit model, we pick the most parsimonious (least complex) model from this top four. This is the model corresponding with formula `ACC ~ noise * category + reading`, for which we will also fit a parallel model with uncorrected reading score for visual comparison purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_adj = Model(df_comt)\n",
    "model1_adj.fit('ACC ~ noise*cars + noise*faces + adjusted_reading_score',\n",
    "                           **defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_traces = pm.traceplot(model1_adj.backend.trace)\n",
    "plt.savefig('figures/object_recognition_model_traces.png', dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create split-violin plot of posterior densities\n",
    "trace = model1_adj.backend.trace\n",
    "effects = pmu.marginal_effects(trace, simple_effects=['noise', 'faces', 'cars'], interactions=['adjusted_reading_score'])\n",
    "renamed_effects = pmu.rename_vars(effects, varnames_dict={\n",
    "    'Intercept': 'bikes:no noise',\n",
    "    'noise': 'bikes:noise',\n",
    "    'cars': 'cars:no noise',\n",
    "    'faces': 'faces:no noise',\n",
    "    'noise:cars': 'cars:noise',\n",
    "    'noise:faces': 'faces:noise',\n",
    "    'adjusted_reading_score': 'adjusted reading score',\n",
    "})\n",
    "\n",
    "trace_raw = model1.backend.trace\n",
    "effects_raw = pmu.marginal_effects(trace_raw, simple_effects=['noise', 'faces', 'cars'], interactions=['raw_reading_score'])\n",
    "renamed_effects_raw = pmu.rename_vars(effects_raw, varnames_dict={\n",
    "    'Intercept': 'bikes:no noise',\n",
    "    'noise': 'bikes:noise',\n",
    "    'cars': 'cars:no noise',\n",
    "    'faces': 'faces:no noise',\n",
    "    'noise:cars': 'cars:noise',\n",
    "    'noise:faces': 'faces:noise',\n",
    "    'raw_reading_score': 'raw reading score',\n",
    "})\n",
    "\n",
    "df1 = pmu.trace_to_df(renamed_effects)\n",
    "df1['model'] = 'adjusted reading'\n",
    "\n",
    "df2 = pmu.trace_to_df(renamed_effects_raw)\n",
    "df2['model'] = 'raw reading'\n",
    "\n",
    "df_traces = pd.concat([df1, df2])\n",
    "df_traces['variable'] = df_traces['variable'].str.replace('adjusted reading score', 'reading score').replace('raw reading score', 'reading score')\n",
    "\n",
    "rcParams.update({'figure.autolayout': False})\n",
    "g = sns.catplot(kind='violin', data=df_traces,\n",
    "                y='variable', x='value', hue='model',\n",
    "                split=True, cut=0, inner='quartile', order=sorted(df_traces['variable'].unique()))\n",
    "g.set(xlabel='', ylabel='')\n",
    "g._legend.set_title('model', prop={'size': 11})\n",
    "g.axes[0][0].axvline(0, color='black', linestyle=':')\n",
    "plt.savefig('figures/split_violins.pdf')\n",
    "plt.savefig('figures/split_violins.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pmu.summary(renamed_effects).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pmu.summary(renamed_effects_raw).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_marginal = pmu.plot_effects(renamed_effects,\n",
    "                              cols=['no noise', 'noise'],\n",
    "                              hues=['bikes', 'cars', 'faces'],\n",
    "                              main='adjusted reading score')\n",
    "plt.savefig('figures/effects.pdf')\n",
    "plt.savefig('figures/effects.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some differences visible between the noise and no noise conditions for the different conditions. Computing the differences in the posterior estimates for these conditions will give us an idea of how robust the differences are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_effects.add_values({\n",
    "    'bikes:noise - bikes:no noise': renamed_effects['bikes:noise'] - renamed_effects['bikes:no noise'],\n",
    "    'cars:noise - cars:no noise': renamed_effects['cars:noise'] - renamed_effects['cars:no noise'],\n",
    "    'faces:noise - faces:no noise': renamed_effects['faces:noise'] - renamed_effects['faces:no noise'],\n",
    "})\n",
    "display(pmu.summary(renamed_effects).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing to check is whether including each participants' ravens and span intercepts as predictors in the model affects our estimate for the effect of adjusted reading score. (We're not really expecting this, because we've taken care to make sure the span and ravens intercepts have no shared variance with the adjusted reading score.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1_full = Model(df_comt)\n",
    "model1_full.fit('ACC ~ noise*cars + noise*faces + adjusted_reading_score + ravens_intercept + span_intercept',\n",
    "                           **defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create split-violin plot of posterior densities\n",
    "trace_full = model1_full.backend.trace\n",
    "effects_full = pmu.marginal_effects(trace_full, simple_effects=['noise', 'faces', 'cars'], interactions=['adjusted_reading_score', 'ravens_intercept', 'span_intercept'])\n",
    "renamed_effects_full = pmu.rename_vars(effects_full, varnames_dict={\n",
    "    'Intercept': 'bikes:no noise',\n",
    "    'noise': 'bikes:noise',\n",
    "    'cars': 'cars:no noise',\n",
    "    'faces': 'faces:no noise',\n",
    "    'noise:cars': 'cars:noise',\n",
    "    'noise:faces': 'faces:noise',\n",
    "    'adjusted_reading_score': 'adjusted reading score',\n",
    "})\n",
    "\n",
    "df3 = pmu.trace_to_df(renamed_effects_full)\n",
    "df3['model'] = 'adjusted reading full'\n",
    "\n",
    "df_traces = pd.concat([df1, df3])\n",
    "df_traces['variable'] = df_traces['variable'].str.replace('adjusted reading score', 'reading score').replace('raw reading score', 'reading score')\n",
    "\n",
    "rcParams.update({'figure.autolayout': False})\n",
    "g = sns.catplot(kind='violin', data=df_traces,\n",
    "                y='variable', x='value', hue='model',\n",
    "                split=True, cut=0, inner='quartile', order=sorted(df_traces['variable'].unique()))\n",
    "g.set(xlabel='', ylabel='')\n",
    "g._legend.set_title('model', prop={'size': 11})\n",
    "g.axes[0][0].axvline(0, color='black', linestyle=':')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, ravens and span are both associated with better recognition memory, but this does not appear to affect our estimate for adjusted reading score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pmu.summary(renamed_effects_full).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The effect of adjusted reading score is still small, but it is worth noting that our correction procedure was very stringent, and our parameter estimate for adjusted reading score likely represents a _lower bound_ for the true effect of learning to read. The true effect probably lies somewhere between that of adjusted and unadjusted reading score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
