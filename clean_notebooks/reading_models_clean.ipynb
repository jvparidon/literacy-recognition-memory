{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display plots inline\n",
    "%matplotlib notebook\n",
    "\n",
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc3 as pm\n",
    "from bambi import Model, Prior\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats\n",
    "import pymc3_utils as pmu\n",
    "\n",
    "# suppress system warnings for legibility\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# resize plots to fit labels inside bounding box\n",
    "from matplotlib import rcParams\n",
    "rcParams.update({'figure.autolayout': True})\n",
    "\n",
    "# MPI color scheme\n",
    "sns.set(style='white', palette='Set2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word reading\n",
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reading = pd.read_csv('data/tamil_reading.tsv', sep='\\t')\n",
    "df_span = pd.read_csv('data/span_intercepts.tsv', sep='\\t')[['pp', 'raw_span_mean', 'span_intercept']]\n",
    "df_ravens = pd.read_csv('data/ravens_intercepts.tsv', sep='\\t')[['pp', 'raw_ravens_mean', 'ravens_intercept']]\n",
    "df_reading = df_reading.merge(df_ravens, left_on='pp', right_on='pp')\n",
    "df_reading = df_reading.merge(df_span, left_on='pp', right_on='pp')\n",
    "display(df_reading.head().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data mangling and plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We suspect reading scores are highly correlated with our cognitive ability measures (ravens and digit span). To get a better idea of these relationships, we will draw a heatmap of absolute (rectified) correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check correlations\n",
    "corrs = df_reading[[\n",
    "    'word', 'pseudoword',\n",
    "    'raw_span_mean', 'span_intercept',\n",
    "    'raw_ravens_mean', 'ravens_intercept']].corr().round(2)\n",
    "display(corrs)\n",
    "diag = np.eye(*corrs.shape)\n",
    "diag[diag == 1] = np.nan\n",
    "corrs = np.abs(corrs + diag)\n",
    "g = sns.heatmap(corrs, cmap='magma', vmin=0, vmax=1, annot=True)\n",
    "g.set_xticklabels(\n",
    "    g.get_xticklabels(),\n",
    "    rotation=45, ha='right')\n",
    "g.set_yticklabels(\n",
    "    g.get_yticklabels(),\n",
    "    rotation=0)\n",
    "g.set(ylim=(len(corrs), 0))\n",
    "g.set_title('absolute correlations between predictors', pad=12)\n",
    "plt.savefig('figures/precorrection_heatmap.pdf')\n",
    "plt.savefig('figures/precorrection_heatmap.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to use reading score as a predictor of literacy, independent of cognitive ability (and other relevant factors that were likely unintentionally captured in the cognitive ability scores, such as familiarity with formal test-taking) we will need to correct the reading scores. Our correction procedure consists of regressing out common variance with the cognitive ability measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reading['literate'] = df_reading['literate'].replace({'y': 'literate', 'low': 'low-literate', 'n': 'illiterate'})\n",
    "df_reading = pd.melt(df_reading, id_vars=['subject', 'pp', 'literate',\n",
    "                                          'raw_ravens_mean', 'ravens_intercept',\n",
    "                                          'raw_span_mean', 'span_intercept'],\n",
    "                     value_vars=['word', 'pseudoword'], var_name='task', value_name='score')\n",
    "display(df_reading.head().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm that the reading scores is indeed associated with literacy in a meaningful way, we are plotting the distribution of word and pseudoword reading scores in each self-reported literacy group (literate, low-literate, and illiterate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot reading score distributions\n",
    "g = sns.catplot(x='task', y='score', hue='literate',\n",
    "                hue_order=['illiterate', 'low-literate', 'literate'],\n",
    "                scale='count', cut=0, inner='quartile',\n",
    "                kind='violin', data=df_reading, legend=False)\n",
    "g.set(ylim=(0, 100), xlabel='', ylabel='number of words read correctly')\n",
    "g.ax.legend(loc='upper right', frameon=False)\n",
    "plt.savefig('figures/reading_scores.pdf')\n",
    "plt.savefig('figures/reading_scores.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-reported literacy is strongly associated with word reading scores, but there is clearly overlap between the categories (i.e., some participants reported being low-literate, but scored better than other participants that reported being fully literate) which is likely due to participants finding it difficult to estimate their own literacy relative to others. We will therefore use the measured word reading scores as our measure of literacy in the analyses reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize variables\n",
    "df_reading['task_z'] = pmu.standardize(pd.get_dummies(df_reading['task'])['word'])\n",
    "df_reading['ravens_z'] = pmu.standardize(df_reading['raw_ravens_mean'])\n",
    "df_reading['span_z'] = pmu.standardize(df_reading['raw_span_mean'])\n",
    "display(df_reading.head().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because word reading scores range from 0 to 100 (with hard boundaries) it would be incorrect to model them using a linear regression model. Instead we are modelling each word in the reading task as a Bernoulli trial, using a generalized linear (i.e., logistic) model. In order to make this possible, we are going to expand each participants' reading score into 100 trials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# expand binomial dataset to bernoulli trials for modeling\n",
    "df_bernoulli = pmu.expand_binomial(df_reading, 'score', 100)\n",
    "display(df_bernoulli.head().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default model params\n",
    "defaults = {\n",
    "    'samples': 5000,\n",
    "    'tune': 2500,\n",
    "    'chains': 4,\n",
    "    'init': 'advi+adapt_diag',\n",
    "    'family': 'bernoulli',\n",
    "    'priors': {'fixed': 'narrow', 'random': 'narrow'},\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expect a model with task (word versus pseudoword), digit span intercept, and ravens intercept as predictors to fit best, but we will also compare models with subsets of those predictors to see if a more parsimonious model perhaps has just as good a fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_intercept = Model(df_bernoulli)\n",
    "fixed_intercept.fit('score_bernoulli ~ 1',\n",
    "                    **defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_task = Model(df_bernoulli)\n",
    "fixed_task.fit('score_bernoulli ~ task_z',\n",
    "               **defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_task_span = Model(df_bernoulli)\n",
    "fixed_task_span.fit('score_bernoulli ~ task_z + span_z',\n",
    "                    **defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_task_ravens = Model(df_bernoulli)\n",
    "fixed_task_ravens.fit('score_bernoulli ~ task_z + ravens_z',\n",
    "                      **defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_task_span_ravens = Model(df_bernoulli)\n",
    "fixed_task_span_ravens.fit('score_bernoulli ~ task_z + span_z + ravens_z',\n",
    "                           **defaults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model comparison of fixed-effects models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_comparison, comparison = pmu.compare([\n",
    "    fixed_intercept,\n",
    "    fixed_task,\n",
    "    fixed_task_span,\n",
    "    fixed_task_ravens,\n",
    "    fixed_task_span_ravens,\n",
    "], ic='LOO')\n",
    "display(comparison)\n",
    "plt.savefig('figures/reading_model_comparison.pdf')\n",
    "plt.savefig('figures/reading_model_comparison.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the `reading_score ~ task + span + ravens` model fits best. We will now refit this model with by-participant intercepts in order to be able to use these intercepts as corrected reading scores in our recognition memory models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best fit fixed effects model with random intercepts by participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_task_span_ravens = Model(df_bernoulli)\n",
    "mixed_task_span_ravens.fit('score_bernoulli ~ task_z + span_z + ravens_z',\n",
    "                      random=['1|pp'],\n",
    "                      **defaults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pmu.summary(mixed_task_span_ravens.backend.trace).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUTS warns that the number of effective samples is smaller than 10% for some parameters. This can be an indicator of several sampling issues, most likely mild autocorrelation. The sampling stats still show more than 1.5K effective samples for all parameters and \\\\(\\hat{r}\\\\) values also look good.  \n",
    "We will plot model traces below to look at the autocorrelation issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting model traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_traces = pm.traceplot(mixed_task_span_ravens.backend.trace)\n",
    "plt.savefig('figures/reading_model_traces.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior traces (right column) show signs of very mild autocorrelation, but nothing pathological. (We appear to be exploring the parameter space fairly well.)  \n",
    "Posterior densities (left column) also look well-behaved (i.e. unimodal, roughly normally distributed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps = df_reading['pp'].unique()\n",
    "pp_nums = [f'1|pp__{i}' for i in range(len(pps))]\n",
    "df_intercepts = pmu.summary(mixed_task_span_ravens.backend.trace).loc[pp_nums]\n",
    "df_intercepts['pp'] = np.sort(pps)\n",
    "\n",
    "display(df_intercepts.head().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_uncorrected = df_reading.groupby('pp', as_index=False).mean().rename(columns={'score': 'raw_reading_score'})\n",
    "df_intercepts = df_intercepts[['pp', 'mode']].rename(columns={'mode': 'adjusted_reading_score'})\n",
    "df_intercepts = df_intercepts.merge(df_uncorrected,\n",
    "                                    left_on='pp', right_on='pp').reset_index()\n",
    "\n",
    "display(df_intercepts.head().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write intercepts to file\n",
    "df_intercepts.to_csv('data/reading_intercepts.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure our score correction had the desired effect, we plot a heatmap of absolute correlations between the cognitive ability measures and both the corrected and uncorrected reading scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check correlations\n",
    "corrs = df_intercepts[[\n",
    "    'raw_reading_score',\n",
    "    'raw_span_mean',\n",
    "    'raw_ravens_mean',\n",
    "    'adjusted_reading_score',\n",
    "]].corr().round(2)\n",
    "display(corrs)\n",
    "upper = np.triu(np.ones(corrs.shape))\n",
    "upper[upper == 1] = np.nan\n",
    "corrs = np.abs(corrs + upper)\n",
    "plt.figure(figsize=(5, 3.5))\n",
    "g = sns.heatmap(corrs, cmap='magma', vmin=0, vmax=1, annot=True)\n",
    "labels = [\n",
    "    'raw reading score',\n",
    "    'digit span',\n",
    "    'raven\\'s score',\n",
    "    'adjusted reading score',\n",
    "]\n",
    "g.set_xticklabels(\n",
    "    labels,\n",
    "    rotation=30, ha='right')\n",
    "g.set_yticklabels(\n",
    "    labels,\n",
    "    rotation=0)\n",
    "g.set(ylim=(len(corrs), 1), xlim=(0, len(corrs) - 1))\n",
    "g.set_title('absolute correlations between predictors', pad=12)\n",
    "plt.savefig('figures/postcorrection_heatmap.pdf')\n",
    "plt.savefig('figures/postcorrection_heatmap.png', dpi=600, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation between unadjusted reading and both cognitive ability measures was .51, after correction the correlation has been reduced to less than .05, while unadjusted and adjusted reading score still share around 50% of their variance (r = .71)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
